{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3aca13",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Step 7: Named Entity Recognition (NER)\n",
    "\n",
    "Concept: Extracting entities like names, places, organizations, dates, etc.\n",
    "\n",
    "âœ… Weâ€™ll learn:\n",
    "\n",
    "NER using spaCy\n",
    "\n",
    "Visualizing entities in text (displacy)\n",
    "\n",
    "NLTK has a built-in chunker for entities, but itâ€™s rule-based (less accurate than SpaCy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd04436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     c:\\Users\\jsril\\anaconda3\\envs\\nlp_env\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     c:\\Users\\jsril\\anaconda3\\envs\\nlp_env\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f218596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1b731",
   "metadata": {},
   "source": [
    "Additional Info:\n",
    "\n",
    "NLTK's ne_chunk() uses a maximum entropy classifier, which requires NumPy for its internal computations.\n",
    "NumPy, a required dependency for NLTK's Named Entity Recognition (NER) model, need to install in environment. NLTK's NER chunker relies on NumPy for some internal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabebbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Apple/NNP)\n",
      "  is/VBZ\n",
      "  looking/VBG\n",
      "  at/IN\n",
      "  buying/VBG\n",
      "  U.K./NNP\n",
      "  startup/NN\n",
      "  for/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# Tokenize and POS tag\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# POS tagging first\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Named Entity Recognition\n",
    "chunks = ne_chunk(pos_tags)\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35256351",
   "metadata": {},
   "source": [
    "# OVerall code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c141976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#step1 tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#step 2 stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#step 3 stemming and lemma\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "#step 4 regex\n",
    "import re\n",
    "\n",
    "#pip install emoji \n",
    "# step 5\n",
    "import emoji\n",
    "\n",
    "# step 6\n",
    "from nltk import pos_tag\n",
    "\n",
    "#step 7\n",
    "from nltk import ne_chunk\n",
    "\n",
    "def download_packages(): \n",
    "    #downloading the required packages for processing\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "    #stop words\n",
    "    nltk.download(\"stopwords\")\n",
    "    # stemming and lemmetization\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "    #POS and tagging \n",
    "    #POS\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    # tagging\n",
    "    nltk.download('tagsets_json')\n",
    "\n",
    "\n",
    "    # named entity recognition package\n",
    "    nltk.download('maxent_ne_chunker_tab')\n",
    "    # nltk.download('maxent_ne_chunker')\n",
    "    nltk.download('words')\n",
    "\n",
    "# packages_names = []\n",
    "# for package in packages_names:\n",
    "#     nltk.download(package)\n",
    "\n",
    "text = \"\"\"\"Hey! ðŸ‘‹ I just found this cool website ðŸ“±: https://www.example.com\n",
    " that has amazing articles on AI ðŸ¤–. I think @john_doe should check it out! Itâ€™s a great read, especially for tech enthusiasts like him. Have you seen the latest article by Dr. Alice Smith on the future of robotics? ðŸš€ I can't wait to learn more! ðŸ” Also, donâ€™t forget to follow their social media for updates ðŸŒ.\"\"\"\n",
    "sent_tokens = sent_tokenize(text)\n",
    "words_tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Sentence Tokens:\")\n",
    "print(sent_tokens)\n",
    "\n",
    "print(\"\\nWord Tokens:\")\n",
    "print(words_tokens)\n",
    "\n",
    "stopping_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in text if word not in stopping_words]\n",
    "\n",
    "print(\"\\nFiltered Words (After Stopword Removal):\")\n",
    "print(filtered_words)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "print(\"\\nStemming Process:\")\n",
    "stemmed_words = []\n",
    "for w in filtered_words:\n",
    "    print(w, \" --> \", ps.stem(w))\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Stemming words:\\n\", stemmed_words)\n",
    "\n",
    "print(\"\\nLemmatization Process:\")\n",
    "lemma_words = []\n",
    "for w in filtered_words:\n",
    "    print(w, \" --> \", wnl.lemmatize(w))\n",
    "    lemma_words.append(wnl.lemmatize(w))\n",
    "\n",
    "print(\"Lemmatized words:\\n\", lemma_words)\n",
    "\n",
    "# step 4 remove punctuations, extra spaces, lower case the sentences\n",
    "listto_sentences = ' '.join(lemma_words)\n",
    "\n",
    "lowercase_sentences = listto_sentences.lower()\n",
    "\n",
    "print(\"\\nLowercase Sentences:\")\n",
    "print(lowercase_sentences)\n",
    "\n",
    "# Remove numbers and punctuation\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', lowercase_sentences)\n",
    "\n",
    "print(\"\\nCleaned Text (No Punctuation, No Numbers):\")\n",
    "print(clean_text)\n",
    "\n",
    "# Remove extra spaces\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "\n",
    "print(\"\\nCleaned Text (No Extra Spaces):\")\n",
    "print(clean_text)\n",
    "\n",
    "# step 5 removing emoji, url, tags& mentions\n",
    "\n",
    "clean_text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "print(\"\\nCleaned Text (After Removing Emojis, URLs, Mentions):\")\n",
    "print(clean_text)\n",
    "\n",
    "# step 6\n",
    "# tokenize the clean_text sentence \n",
    "text_tokens = word_tokenize(clean_text)\n",
    "\n",
    "print(\"\\nTokenized Cleaned Text:\")\n",
    "print(text_tokens)\n",
    "\n",
    "# parts of speech tagging\n",
    "pos = pos_tag(text_tokens)\n",
    "print(\"\\nPOS Tagging Results:\")\n",
    "print(pos)\n",
    "\n",
    "\n",
    "# Step 7: Named Entity Recognition (NER)\n",
    "# ner_words_token = ne_chunk(text_tokens)  # using tokenized words\n",
    "ner_words = ne_chunk(pos)  # using POS tagged words\n",
    "# print(\"\\nNER Results (Using Tokenized Words):\")\n",
    "# print(ner_words_token)\n",
    "\n",
    "print(\"\\nNER Results (Using POS Tagged Words):\")\n",
    "print(ner_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d810f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Text (No Punctuation, No Numbers):\n",
      "Hey  I just found this cool website  httpswwwexamplecom                                                                                                that has amazing articles on AI  with  articles  learning resources Also check out this wwwaiworldcom I think johndoe clarasmith should check it out Its a great read especially for tech enthusiasts like him Have you seen the latest article by Dr Alice Smith on the future of robotics  I cant wait to learn more  Also dont forget to follow their social media for updates artificialintelligence nextgenai hopeai genai aiworld \n",
      "\n",
      "Cleaned Text (No Extra Spaces):\n",
      "Hey I just found this cool website httpswwwexamplecom that has amazing articles on AI with articles learning resources Also check out this wwwaiworldcom I think johndoe clarasmith should check it out Its a great read especially for tech enthusiasts like him Have you seen the latest article by Dr Alice Smith on the future of robotics I cant wait to learn more Also dont forget to follow their social media for updates artificialintelligence nextgenai hopeai genai aiworld\n",
      "\n",
      "Cleaned Text (After Removing Emojis, URLs, Mentions):\n",
      "Hey I just found this cool website  that has amazing articles on AI with articles learning resources Also check out this  I think johndoe clarasmith should check it out Its a great read especially for tech enthusiasts like him Have you seen the latest article by Dr Alice Smith on the future of robotics I cant wait to learn more Also dont forget to follow their social media for updates artificialintelligence nextgenai hopeai genai aiworld\n",
      "\n",
      "Lowercase Sentences:\n",
      "hey i just found this cool website  that has amazing articles on ai with articles learning resources also check out this  i think johndoe clarasmith should check it out its a great read especially for tech enthusiasts like him have you seen the latest article by dr alice smith on the future of robotics i cant wait to learn more also dont forget to follow their social media for updates artificialintelligence nextgenai hopeai genai aiworld\n",
      "Sentence Tokens:\n",
      "['\"Hey!', 'ðŸ‘‹ I just found this cool website ðŸ“±: https://www.example.com                                                                                                that has amazing articles on AI ðŸ¤– with 100 articles 30 learning resources.', 'Also check out this www.aiworld.com.', 'I think @john_doe @clara_smith002 should check it out!', 'Itâ€™s a great read, especially for tech enthusiasts like him.', 'Have you seen the latest article by Dr. Alice Smith on the future of robotics?', \"ðŸš€ I can't wait to learn more!\", 'ðŸ” Also, donâ€™t forget to follow their social media for updates #artificialintelligence #nextgenai #hopeai #genai #aiworld ðŸŒ.']\n",
      "\n",
      "Word Tokens:\n",
      "['hey', 'i', 'just', 'found', 'this', 'cool', 'website', 'that', 'has', 'amazing', 'articles', 'on', 'ai', 'with', 'articles', 'learning', 'resources', 'also', 'check', 'out', 'this', 'i', 'think', 'johndoe', 'clarasmith', 'should', 'check', 'it', 'out', 'its', 'a', 'great', 'read', 'especially', 'for', 'tech', 'enthusiasts', 'like', 'him', 'have', 'you', 'seen', 'the', 'latest', 'article', 'by', 'dr', 'alice', 'smith', 'on', 'the', 'future', 'of', 'robotics', 'i', 'cant', 'wait', 'to', 'learn', 'more', 'also', 'dont', 'forget', 'to', 'follow', 'their', 'social', 'media', 'for', 'updates', 'artificialintelligence', 'nextgenai', 'hopeai', 'genai', 'aiworld']\n",
      "\n",
      "Filtered Words (After Stopword Removal):\n",
      "['hey', 'found', 'cool', 'website', 'amazing', 'articles', 'ai', 'articles', 'learning', 'resources', 'also', 'check', 'think', 'johndoe', 'clarasmith', 'check', 'great', 'read', 'especially', 'tech', 'enthusiasts', 'like', 'seen', 'latest', 'article', 'dr', 'alice', 'smith', 'future', 'robotics', 'cant', 'wait', 'learn', 'also', 'dont', 'forget', 'follow', 'social', 'media', 'updates', 'artificialintelligence', 'nextgenai', 'hopeai', 'genai', 'aiworld']\n",
      "\n",
      "Stemming Process:\n",
      "hey  -->  hey\n",
      "found  -->  found\n",
      "cool  -->  cool\n",
      "website  -->  websit\n",
      "amazing  -->  amaz\n",
      "articles  -->  articl\n",
      "ai  -->  ai\n",
      "articles  -->  articl\n",
      "learning  -->  learn\n",
      "resources  -->  resourc\n",
      "also  -->  also\n",
      "check  -->  check\n",
      "think  -->  think\n",
      "johndoe  -->  johndo\n",
      "clarasmith  -->  clarasmith\n",
      "check  -->  check\n",
      "great  -->  great\n",
      "read  -->  read\n",
      "especially  -->  especi\n",
      "tech  -->  tech\n",
      "enthusiasts  -->  enthusiast\n",
      "like  -->  like\n",
      "seen  -->  seen\n",
      "latest  -->  latest\n",
      "article  -->  articl\n",
      "dr  -->  dr\n",
      "alice  -->  alic\n",
      "smith  -->  smith\n",
      "future  -->  futur\n",
      "robotics  -->  robot\n",
      "cant  -->  cant\n",
      "wait  -->  wait\n",
      "learn  -->  learn\n",
      "also  -->  also\n",
      "dont  -->  dont\n",
      "forget  -->  forget\n",
      "follow  -->  follow\n",
      "social  -->  social\n",
      "media  -->  media\n",
      "updates  -->  updat\n",
      "artificialintelligence  -->  artificialintellig\n",
      "nextgenai  -->  nextgenai\n",
      "hopeai  -->  hopeai\n",
      "genai  -->  genai\n",
      "aiworld  -->  aiworld\n",
      "Stemming words:\n",
      " ['hey', 'found', 'cool', 'websit', 'amaz', 'articl', 'ai', 'articl', 'learn', 'resourc', 'also', 'check', 'think', 'johndo', 'clarasmith', 'check', 'great', 'read', 'especi', 'tech', 'enthusiast', 'like', 'seen', 'latest', 'articl', 'dr', 'alic', 'smith', 'futur', 'robot', 'cant', 'wait', 'learn', 'also', 'dont', 'forget', 'follow', 'social', 'media', 'updat', 'artificialintellig', 'nextgenai', 'hopeai', 'genai', 'aiworld']\n",
      "\n",
      "Lemmatization Process:\n",
      "hey  -->  hey\n",
      "found  -->  found\n",
      "cool  -->  cool\n",
      "website  -->  website\n",
      "amazing  -->  amazing\n",
      "articles  -->  article\n",
      "ai  -->  ai\n",
      "articles  -->  article\n",
      "learning  -->  learning\n",
      "resources  -->  resource\n",
      "also  -->  also\n",
      "check  -->  check\n",
      "think  -->  think\n",
      "johndoe  -->  johndoe\n",
      "clarasmith  -->  clarasmith\n",
      "check  -->  check\n",
      "great  -->  great\n",
      "read  -->  read\n",
      "especially  -->  especially\n",
      "tech  -->  tech\n",
      "enthusiasts  -->  enthusiast\n",
      "like  -->  like\n",
      "seen  -->  seen\n",
      "latest  -->  latest\n",
      "article  -->  article\n",
      "dr  -->  dr\n",
      "alice  -->  alice\n",
      "smith  -->  smith\n",
      "future  -->  future\n",
      "robotics  -->  robotics\n",
      "cant  -->  cant\n",
      "wait  -->  wait\n",
      "learn  -->  learn\n",
      "also  -->  also\n",
      "dont  -->  dont\n",
      "forget  -->  forget\n",
      "follow  -->  follow\n",
      "social  -->  social\n",
      "media  -->  medium\n",
      "updates  -->  update\n",
      "artificialintelligence  -->  artificialintelligence\n",
      "nextgenai  -->  nextgenai\n",
      "hopeai  -->  hopeai\n",
      "genai  -->  genai\n",
      "aiworld  -->  aiworld\n",
      "Lemmatized words:\n",
      " ['hey', 'found', 'cool', 'website', 'amazing', 'article', 'ai', 'article', 'learning', 'resource', 'also', 'check', 'think', 'johndoe', 'clarasmith', 'check', 'great', 'read', 'especially', 'tech', 'enthusiast', 'like', 'seen', 'latest', 'article', 'dr', 'alice', 'smith', 'future', 'robotics', 'cant', 'wait', 'learn', 'also', 'dont', 'forget', 'follow', 'social', 'medium', 'update', 'artificialintelligence', 'nextgenai', 'hopeai', 'genai', 'aiworld']\n",
      "\n",
      "POS Tagging Results:\n",
      "[('hey', 'NN'), ('found', 'VBD'), ('cool', 'JJ'), ('website', 'RB'), ('amazing', 'JJ'), ('article', 'NN'), ('ai', 'NN'), ('article', 'NN'), ('learning', 'VBG'), ('resource', 'NN'), ('also', 'RB'), ('check', 'VB'), ('think', 'NN'), ('johndoe', 'NN'), ('clarasmith', 'NN'), ('check', 'NN'), ('great', 'JJ'), ('read', 'JJ'), ('especially', 'RB'), ('tech', 'JJ'), ('enthusiast', 'NN'), ('like', 'IN'), ('seen', 'VBN'), ('latest', 'JJS'), ('article', 'NN'), ('dr', 'NN'), ('alice', 'NN'), ('smith', 'JJ'), ('future', 'NN'), ('robotics', 'NNS'), ('cant', 'JJ'), ('wait', 'NN'), ('learn', 'NN'), ('also', 'RB'), ('dont', 'VBZ'), ('forget', 'VB'), ('follow', 'JJ'), ('social', 'JJ'), ('medium', 'NN'), ('update', 'JJ'), ('artificialintelligence', 'NN'), ('nextgenai', 'JJ'), ('hopeai', 'NN'), ('genai', 'NN'), ('aiworld', 'NN')]\n",
      "\n",
      "NER Results (Using POS Tagged Words):\n",
      "(S\n",
      "  hey\n",
      "  found\n",
      "  cool\n",
      "  website\n",
      "  amazing\n",
      "  article\n",
      "  ai\n",
      "  article\n",
      "  learning\n",
      "  resource\n",
      "  also\n",
      "  check\n",
      "  think\n",
      "  johndoe\n",
      "  clarasmith\n",
      "  check\n",
      "  great\n",
      "  read\n",
      "  especially\n",
      "  tech\n",
      "  enthusiast\n",
      "  like\n",
      "  seen\n",
      "  latest\n",
      "  article\n",
      "  dr\n",
      "  alice\n",
      "  smith\n",
      "  future\n",
      "  robotics\n",
      "  cant\n",
      "  wait\n",
      "  learn\n",
      "  also\n",
      "  dont\n",
      "  forget\n",
      "  follow\n",
      "  social\n",
      "  medium\n",
      "  update\n",
      "  artificialintelligence\n",
      "  nextgenai\n",
      "  hopeai\n",
      "  genai\n",
      "  aiworld)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#step1 tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#step 2 stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#step 3 stemming and lemma\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "#step 4 regex\n",
    "import re\n",
    "\n",
    "#pip install emoji \n",
    "# step 5\n",
    "import emoji\n",
    "\n",
    "# step 6\n",
    "from nltk import pos_tag\n",
    "\n",
    "#step 7\n",
    "from nltk import ne_chunk\n",
    "\n",
    "def download_packages(): \n",
    "    #downloading the required packages for processing\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "    #stop words\n",
    "    nltk.download(\"stopwords\")\n",
    "    # stemming and lemmetization\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "    #POS and tagging \n",
    "    #POS\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    # tagging\n",
    "    nltk.download('tagsets_json')\n",
    "\n",
    "\n",
    "    # named entity recognition package\n",
    "    nltk.download('maxent_ne_chunker_tab')\n",
    "    # nltk.download('maxent_ne_chunker')\n",
    "    nltk.download('words')\n",
    "\n",
    "# packages_names = []\n",
    "# for package in packages_names:\n",
    "#     nltk.download(package)\n",
    "\n",
    "text = \"\"\"\"Hey! ðŸ‘‹ I just found this cool website ðŸ“±: https://www.example.com                                                                                                that has amazing articles on AI ðŸ¤– with 100 articles 30 learning resources. Also check out this www.aiworld.com. I think @john_doe @clara_smith002 should check it out! Itâ€™s a great read, especially for tech enthusiasts like him. Have you seen the latest article by Dr. Alice Smith on the future of robotics? ðŸš€ I can't wait to learn more! ðŸ” Also, donâ€™t forget to follow their social media for updates #artificialintelligence #nextgenai #hopeai #genai #aiworld ðŸŒ.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Remove numbers and punctuation\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "print(\"\\nCleaned Text (No Punctuation, No Numbers):\")\n",
    "print(clean_text)\n",
    "\n",
    "# Remove extra spaces\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "\n",
    "print(\"\\nCleaned Text (No Extra Spaces):\")\n",
    "print(clean_text)\n",
    "\n",
    "\n",
    "# step 5 removing url, twitter tags & mentions,  removing emoji,\n",
    "clean_text = re.sub(r'http\\S+|www\\S+', '', clean_text)\n",
    "\n",
    "clean_text = re.sub(r'#', '', clean_text)\n",
    "\n",
    "clean_text = emoji.replace_emoji(clean_text, replace='')\n",
    "\n",
    "print(\"\\nCleaned Text (After Removing Emojis, URLs, Mentions):\")\n",
    "print(clean_text)\n",
    "#clean_text has all the sentences preprocessed converted to lowercase and then tokenized for further processing\n",
    "lowercase_sentences = clean_text.lower()\n",
    "\n",
    "\n",
    "print(\"\\nLowercase Sentences:\")\n",
    "print(lowercase_sentences)\n",
    "\n",
    "sent_tokens = sent_tokenize(text)\n",
    "words_tokens = word_tokenize(lowercase_sentences)\n",
    "\n",
    "print(\"Sentence Tokens:\")\n",
    "print(sent_tokens)\n",
    "\n",
    "print(\"\\nWord Tokens:\")\n",
    "print(words_tokens)\n",
    "\n",
    "stopping_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in words_tokens if word not in stopping_words]\n",
    "\n",
    "print(\"\\nFiltered Words (After Stopword Removal):\")\n",
    "print(filtered_words)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "print(\"\\nStemming Process:\")\n",
    "stemmed_words = []\n",
    "for w in filtered_words:\n",
    "    print(w, \" --> \", ps.stem(w))\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Stemming words:\\n\", stemmed_words)\n",
    "\n",
    "print(\"\\nLemmatization Process:\")\n",
    "lemma_words = []\n",
    "for w in filtered_words:\n",
    "    print(w, \" --> \", wnl.lemmatize(w))\n",
    "    lemma_words.append(wnl.lemmatize(w))\n",
    "\n",
    "print(\"Lemmatized words:\\n\", lemma_words)\n",
    "\n",
    "join_words = ' '.join(lemma_words)\n",
    "\n",
    "# parts of speech tagging\n",
    "pos = pos_tag(lemma_words)\n",
    "print(\"\\nPOS Tagging Results:\")\n",
    "print(pos)\n",
    "\n",
    "\n",
    "# Step 7: Named Entity Recognition (NER)\n",
    "# ner_words_token = ne_chunk(text_tokens)  # using tokenized words\n",
    "ner_words = ne_chunk(lemma_words)  # using POS tagged words\n",
    "# print(\"\\nNER Results (Using Tokenized Words):\")\n",
    "# print(ner_words_token)\n",
    "\n",
    "print(\"\\nNER Results (Using POS Tagged Words):\")\n",
    "print(ner_words)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
