{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c531676f",
   "metadata": {},
   "source": [
    "# Task: Text Normalization Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6502d72",
   "metadata": {},
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d1976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural Language Processing helps computers understand, interpret, and generate human language. \n",
    "It involves techniques like tokenization, stemming, and lemmatization to clean text data efficiently.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad3b3c",
   "metadata": {},
   "source": [
    "Requirements\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')  # for tokenizers\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\") # for stopwords\n",
    "nlk.download(\"wordnet\") # for stemming and lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef86b1",
   "metadata": {},
   "source": [
    "## Tokenize the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd692c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['Natural Language Processing helps computers understand, interpret, and generate human language.', 'It involves techniques like tokenization, stemming, and lemmatization to clean text data efficiently.']\n",
      "Word Tokenization: ['Natural', 'Language', 'Processing', 'helps', 'computers', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'It', 'involves', 'techniques', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'to', 'clean', 'text', 'data', 'efficiently', '.']\n",
      "Word Punct Tokenization: Natural Language Processing helps computers understand, interpret, and generate human language. \n",
      "It involves techniques like tokenization, stemming, and lemmatization to clean text data efficiently.\n",
      "\n",
      "Total words before preprocessing: 30\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\", sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words_tokens = word_tokenize(text)\n",
    "print(\"Word Tokenization:\", words_tokens)\n",
    "\n",
    "word_punctuation = wordpunct_tokenize(text)\n",
    "print(\"Word Punct Tokenization:\", text)\n",
    "\n",
    "\n",
    "print(\"Total words before preprocessing:\", len(words_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232cc88",
   "metadata": {},
   "source": [
    "## Remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88258997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stopword removal: 26\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwrd =set(stopwords.words(\"english\"))\n",
    "# print(\"STOP WORDS IN NLTK :\\n\", stopwrd)\n",
    "filtered_words = [ word  for word in words_tokens if word.lower() not in stopwrd ]\n",
    "print(\"After stopword removal:\", len(filtered_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324fd923",
   "metadata": {},
   "source": [
    "## Apply stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee323843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural  -->  natur\n",
      "Language  -->  languag\n",
      "Processing  -->  process\n",
      "helps  -->  help\n",
      "computers  -->  comput\n",
      "understand  -->  understand\n",
      ",  -->  ,\n",
      "interpret  -->  interpret\n",
      ",  -->  ,\n",
      "generate  -->  gener\n",
      "human  -->  human\n",
      "language  -->  languag\n",
      ".  -->  .\n",
      "involves  -->  involv\n",
      "techniques  -->  techniqu\n",
      "like  -->  like\n",
      "tokenization  -->  token\n",
      ",  -->  ,\n",
      "stemming  -->  stem\n",
      ",  -->  ,\n",
      "lemmatization  -->  lemmat\n",
      "clean  -->  clean\n",
      "text  -->  text\n",
      "data  -->  data\n",
      "efficiently  -->  effici\n",
      ".  -->  .\n",
      "Stemmed words: ['natur', 'languag', 'process', 'help', 'comput', 'understand', ',', 'interpret', ',', 'gener', 'human', 'languag', '.', 'involv', 'techniqu', 'like', 'token', ',', 'stem', ',', 'lemmat', 'clean', 'text', 'data', 'effici', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = []\n",
    "\n",
    "for w in filtered_words:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "    print(w, \" --> \", ps.stem(w))\n",
    "\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f190f",
   "metadata": {},
   "source": [
    "## Apply lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1559478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural  -->  Natural\n",
      "Language  -->  Language\n",
      "Processing  -->  Processing\n",
      "helps  -->  help\n",
      "computers  -->  computer\n",
      "understand  -->  understand\n",
      ",  -->  ,\n",
      "interpret  -->  interpret\n",
      ",  -->  ,\n",
      "generate  -->  generate\n",
      "human  -->  human\n",
      "language  -->  language\n",
      ".  -->  .\n",
      "involves  -->  involves\n",
      "techniques  -->  technique\n",
      "like  -->  like\n",
      "tokenization  -->  tokenization\n",
      ",  -->  ,\n",
      "stemming  -->  stemming\n",
      ",  -->  ,\n",
      "lemmatization  -->  lemmatization\n",
      "clean  -->  clean\n",
      "text  -->  text\n",
      "data  -->  data\n",
      "efficiently  -->  efficiently\n",
      ".  -->  .\n",
      "Lemmatized words: ['Natural', 'Language', 'Processing', 'help', 'computer', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '.', 'involves', 'technique', 'like', 'tokenization', ',', 'stemming', ',', 'lemmatization', 'clean', 'text', 'data', 'efficiently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "ws = WordNetLemmatizer()\n",
    "lemma_words = []\n",
    "for w in filtered_words:\n",
    "    print(w, \" --> \", ws.lemmatize(w))\n",
    "    lemma_words.append(ws.lemmatize(w))\n",
    "\n",
    "print(\"Lemmatized words:\", lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24590b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing help computer understand , interpret , generate human language . involves technique like tokenization , stemming , lemmatization clean text data efficiently .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "' '.join(lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4c0c1",
   "metadata": {},
   "source": [
    "stemmed_words, lemmatized_words generated differently that are not in word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46d6d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'help', 'comput', 'gener', 'languag', 'involv', 'techniqu', 'token', 'stem', 'lemmat', 'effici']\n"
     ]
    }
   ],
   "source": [
    "unique_stem = [word for word in stemmed_words if word not in filtered_words]\n",
    "print(unique_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bac93bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'computer', 'technique']\n"
     ]
    }
   ],
   "source": [
    "unique_lemma = [word for word in lemma_words if word not in filtered_words]\n",
    "print(unique_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994258ff",
   "metadata": {},
   "source": [
    "# Unique tokens in stemming and lemma,  count distinct words only, ignoring duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6caf2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens after stemming: 21\n",
      "Unique tokens after lemmatization: 22\n"
     ]
    }
   ],
   "source": [
    "# Unique stemmed tokens\n",
    "unique_stemmed_tokens = set(stemmed_words)\n",
    "print(\"Unique tokens after stemming:\", len(unique_stemmed_tokens))\n",
    "\n",
    "# Unique lemmatized tokens\n",
    "unique_lemmatized_tokens = set(lemma_words)\n",
    "print(\"Unique tokens after lemmatization:\", len(unique_lemmatized_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ef814",
   "metadata": {},
   "source": [
    "## Compare\n",
    "\n",
    "Which version (stemmed or lemmatized) gives more readable words?\n",
    "- lemmatized words are more readable than stemming\n",
    "\n",
    "How many unique tokens remain after both?\n",
    "- Unique tokens after stemming: 21\n",
    "- Unique tokens after lemmatization: 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
