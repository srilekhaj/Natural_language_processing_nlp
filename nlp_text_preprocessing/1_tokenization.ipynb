{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcc00ed",
   "metadata": {},
   "source": [
    "# üîπ Step 1: Tokenization\n",
    "\n",
    "Concept: Splitting text into smaller meaningful units (words or sentences).\n",
    "\n",
    "‚úÖ We‚Äôll cover:\n",
    "\n",
    "- Sentence tokenization\n",
    "\n",
    "- Word tokenization\n",
    "\n",
    "- Difference between NLTK and spaCy tokenizers\n",
    "\n",
    "\n",
    "NLTK has built-in tokenizers:\n",
    "\n",
    "sent_tokenize() ‚Üí for sentence-level splitting\n",
    "\n",
    "word_tokenize() ‚Üí for word-level splitting\n",
    "\n",
    "************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c2566",
   "metadata": {},
   "source": [
    "# Downloading the tokenizer module=(single py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3077696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # for tokenizers\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3012ec9",
   "metadata": {},
   "source": [
    "### üìù Note:\n",
    "### word_tokenize keeps punctuation as separate tokens (which can be useful later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113132b9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
